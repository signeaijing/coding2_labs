{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi-cnJAXiF8k"
      },
      "source": [
        "PROMPT: \n",
        "Working in groups, build a simple webscraper that scrapes a bunch of documents from the internet and creates a summary of each one. One way to do this is to start with a bunch of pages, and then follow all the links on those pages, scraping new documents as you go. Then you can record each link with a quick summary in a list.\n",
        "\n",
        "You can use any method you like to do the summary - you could just select a couple of sentences, or use some form of summarisation tool (Huggingface has one, Gensim might have one, or you could make one up).\n",
        "\n",
        "If you manage to achieve this, extract keywords by counting the total number of similar words from all the different documents and see if any are more popular than others. Search for documents that contain those keywords using Python and then summarise those documents too!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1pDXJB7gQm5"
      },
      "outputs": [],
      "source": [
        "import bs4 \n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytgLbcG0gQnB",
        "outputId": "71bb2e93-6b09-4f7f-9236-de6ca5f9ec02"
      },
      "outputs": [],
      "source": [
        "# putting the url in variable, 'url'\n",
        "absolute_url = 'https://en.m.wikipedia.org'\n",
        "relative_url = '/wiki/Dragon'      \n",
        "url= absolute_url + relative_url          # scraping from the wikipedia dragon page \n",
        "\n",
        "# get the url \n",
        "r = requests.get(url)\n",
        "\n",
        "# check the status\n",
        "print(\"Status code - \", r.status_code)\n",
        "#print(r.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL7tnLQcgQnD"
      },
      "outputs": [],
      "source": [
        "# create the soup! \n",
        "soup = BeautifulSoup(r.content,'html.parser')\n",
        "#soup.prettify()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMuN1cDDgQnD",
        "outputId": "f4d0dad5-1bec-4dbb-f5f4-10ed536c8ec3"
      },
      "outputs": [],
      "source": [
        "# list to hold the scraped links\n",
        "urls = []\n",
        "\n",
        "# iterate through the main page and get href for each page and put in list\n",
        "for url in soup.find_all('a'):       # find all a tags containing hyper links\n",
        "    href = url.get('href')           # get the href for each hyperlink on the website \n",
        "    url = absolute_url+href           \n",
        "    urls.append(url)                 # append full link to the list of urls\n",
        "urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y68olvmzHXg"
      },
      "outputs": [],
      "source": [
        "# look for p tags in all the urls\n",
        "for url in urls[500:510]:       # making the list smaller \n",
        "  paragraphs = soup.find_all('p')\n",
        "\n",
        "# hold the pages \n",
        "paragraphs_list=[]\n",
        "\n",
        "# get content for each page\n",
        "for paragraph in paragraphs:\n",
        "  elements = paragraph.text\n",
        "  paragraphs_list.append(paragraph.text)\n",
        "  #print(paragraph.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXHi5Tqh2t7m",
        "outputId": "7dd796e9-28dc-4639-ae07-28a1817488f0"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9hj2x63gQnG"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUkv98cUcxk-"
      },
      "outputs": [],
      "source": [
        "# instantiating the summarizer \n",
        "summarizer = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\",min_length=5,max_length=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY84MClRd5BY",
        "outputId": "d6392d34-7d3a-492c-8ea5-4e8fd41a3199"
      },
      "outputs": [],
      "source": [
        "# summarize the pages!\n",
        "for paragraph in paragraphs_list:\n",
        "  print(summarizer(paragraph))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "fff1c07074595ec3cf02af86b27d7257e33a586763b7ad58c7bb002dfbbeba14"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
